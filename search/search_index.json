{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"mpox-seek \ud83d\udd2c An ONT Pipeline for targeted or whole-genome Monkeypox sequencing mpox-seek is an awesome, portable and fast oxford nanopore pipeline for targeted and whole-genome monkeypox sequencing. Overview \u00b6 Welcome to mpox-seek's documentation! This guide is the main source of documentation for users that are getting started with the Monkeypox Nanopore Pipeline . The ./mpox-seek pipeline is composed several inter-related sub commands to setup and run the pipeline across different systems. Each of the available sub commands perform different functions: mpox-seek run Run the mpox-seek pipeline with your input files. mpox-seek unlock Unlocks a previous runs output directory. mpox-seek install Download remote reference files locally. mpox-seek cache Cache remote software containers locally. mpox-seek is a streamlined viral metagenomics pipeline to align, collapse, and visualize targeted or whole-genome monekypox samples. It relies on technologies like Singularity 1 to maintain the highest-level of reproducibility. The pipeline consists of a series of data processing and quality-control steps orchestrated by Snakemake 2 , a flexible and scalable workflow management system, to submit jobs to a cluster. By default, the pipeline will utilize singularity to guarantee the highest level of reproducibility; however, the --use-conda option of the run sub command can be provided to use conda/mamba instead of singularity. If possible, we recommend using singularity over conda for reproducibility; however, it is worth noting that singularity and conda produce identical results for this pipeline. If you plan on running this pipeline on a laptop or desktop computer, we recommend using conda/mamba over singularity. The pipeline is compatible with data generated from Oxford Nanopore sequencing Technologies . As input, it accepts a set of gzipped FastQ files (already basecalled) and can be run locally on a compute instance or on-premise using a cluster. A user can define the method or mode of execution. The pipeline can submit jobs to a cluster using a job scheduler like SLURM (more coming soon!). A hybrid approach ensures the pipeline is accessible to all users. Before getting started, we highly recommend reading through the usage section of each available sub command. For more information about issues or trouble-shooting a problem, please checkout our FAQ prior to opening an issue on Github . Contribute \u00b6 This site is a living document, created for and by members like you. mpox-seek is maintained by the members of OpenOmics and is improved by continous feedback! We encourage you to contribute new content and make improvements to existing content via pull request to our GitHub repository . Citation \u00b6 If you use this software, please cite it as below: BibTex APA @software{Kuhn_OpenOmics_mpox-seek_2024, author = {Skyler Kuhn and Schaughency, Paul}, title = {OpenOmics/mpox-seek: v0.1.0}, month = apr, year = 2024, publisher = {Zenodo}, version = {v0.1.0}, doi = {10.5281/zenodo.10957607}, url = {https://doi.org/10.5281/zenodo.10957607} } Skyler Kuhn, & Schaughency, P. (2024). OpenOmics/mpox-seek: v0.1.0 (v0.1.0). Zenodo. https://doi.org/10.5281/zenodo.10957607 Please do not forget to also cite our methods paper in addition to the zenodo DOI above. For more zenodo citation style options, please visit the pipeline's Zenodo page . References \u00b6 1. Kurtzer GM, Sochat V, Bauer MW (2017). Singularity: Scientific containers for mobility of compute. PLoS ONE 12(5): e0177459. 2. Koster, J. and S. Rahmann (2018). \"Snakemake-a scalable bioinformatics workflow engine.\" Bioinformatics 34(20): 3600.","title":"About"},{"location":"#overview","text":"Welcome to mpox-seek's documentation! This guide is the main source of documentation for users that are getting started with the Monkeypox Nanopore Pipeline . The ./mpox-seek pipeline is composed several inter-related sub commands to setup and run the pipeline across different systems. Each of the available sub commands perform different functions: mpox-seek run Run the mpox-seek pipeline with your input files. mpox-seek unlock Unlocks a previous runs output directory. mpox-seek install Download remote reference files locally. mpox-seek cache Cache remote software containers locally. mpox-seek is a streamlined viral metagenomics pipeline to align, collapse, and visualize targeted or whole-genome monekypox samples. It relies on technologies like Singularity 1 to maintain the highest-level of reproducibility. The pipeline consists of a series of data processing and quality-control steps orchestrated by Snakemake 2 , a flexible and scalable workflow management system, to submit jobs to a cluster. By default, the pipeline will utilize singularity to guarantee the highest level of reproducibility; however, the --use-conda option of the run sub command can be provided to use conda/mamba instead of singularity. If possible, we recommend using singularity over conda for reproducibility; however, it is worth noting that singularity and conda produce identical results for this pipeline. If you plan on running this pipeline on a laptop or desktop computer, we recommend using conda/mamba over singularity. The pipeline is compatible with data generated from Oxford Nanopore sequencing Technologies . As input, it accepts a set of gzipped FastQ files (already basecalled) and can be run locally on a compute instance or on-premise using a cluster. A user can define the method or mode of execution. The pipeline can submit jobs to a cluster using a job scheduler like SLURM (more coming soon!). A hybrid approach ensures the pipeline is accessible to all users. Before getting started, we highly recommend reading through the usage section of each available sub command. For more information about issues or trouble-shooting a problem, please checkout our FAQ prior to opening an issue on Github .","title":"Overview"},{"location":"#contribute","text":"This site is a living document, created for and by members like you. mpox-seek is maintained by the members of OpenOmics and is improved by continous feedback! We encourage you to contribute new content and make improvements to existing content via pull request to our GitHub repository .","title":"Contribute"},{"location":"#citation","text":"If you use this software, please cite it as below: BibTex APA @software{Kuhn_OpenOmics_mpox-seek_2024, author = {Skyler Kuhn and Schaughency, Paul}, title = {OpenOmics/mpox-seek: v0.1.0}, month = apr, year = 2024, publisher = {Zenodo}, version = {v0.1.0}, doi = {10.5281/zenodo.10957607}, url = {https://doi.org/10.5281/zenodo.10957607} } Skyler Kuhn, & Schaughency, P. (2024). OpenOmics/mpox-seek: v0.1.0 (v0.1.0). Zenodo. https://doi.org/10.5281/zenodo.10957607 Please do not forget to also cite our methods paper in addition to the zenodo DOI above. For more zenodo citation style options, please visit the pipeline's Zenodo page .","title":"Citation"},{"location":"#references","text":"1. Kurtzer GM, Sochat V, Bauer MW (2017). Singularity: Scientific containers for mobility of compute. PLoS ONE 12(5): e0177459. 2. Koster, J. and S. Rahmann (2018). \"Snakemake-a scalable bioinformatics workflow engine.\" Bioinformatics 34(20): 3600.","title":"References"},{"location":"examples/","text":"Examples \u00b6 This page is under construction. Please come back later for more information!","title":"Examples"},{"location":"examples/#examples","text":"This page is under construction. Please come back later for more information!","title":"Examples"},{"location":"license/","text":"MIT License \u00b6 Copyright \u00a9 2022 OpenOmics Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#mit-license","text":"Copyright \u00a9 2022 OpenOmics Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"setup/","text":"Dependencies \u00b6 Requirements Using Singularity : singularity>=3.5 snakemake<8.0 Using Conda or Mamba : conda/mamba snakemake<8.0 Snakemake must be installed on the target system. Snakemake is a workflow manager that orchestrates each step of the pipeline. The second dependency, i.e singularity OR conda/mamba , handles the dowloading/installation of any remaining software dependencies. By default, the pipeline will utilize singularity; however, the --use-conda option of the run sub command can be provided to use conda/mamba instead of singularity. If possible, we recommend using singularity over conda for reproducibility; however, it is worth noting that singularity and conda produce identical results for this pipeline. If you are running the pipeline on Windows, please use the Windows Subsystem for Linux (WSL) . Singularity can be installed on WSL following these instructions . With that being said, we recommend using conda/mamba over singularity. It is easier to setup and install via using WSL on Windows. Please see the instructions below to install conda/mamba on your system. You can check to see if mpox-seek's software requirements are met by running: # Check if dependencies # are already installed which snakemake || echo 'Error: snakemake is not install.' which singularity \\ || which conda \\ || which mamba \\ || echo 'Error: singularity or conda or mamba are not installed.' # Install conda/mamba if missing. # Create directory to install # everything in $HOME/pipelines. mkdir -p \" ${ HOME :- ~ } /pipelines/conda\" cd \" ${ HOME :- ~ } /pipelines/conda\" # Download the installer for # miniforge and install conda. curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3- $( uname ) - $( uname -m ) .sh\" # Installs conda, a prompt will # appear, hit ENTER, type yes a # few times, and then enter no # when it asks to initialize # your shell/startup bash \"Miniforge3- $( uname ) - $( uname -m ) .sh\" -f -p $PWD # Start up the conda environment, # if conda/mamba are not installed # follow the instructions above. source \" ${ HOME :- ~ } /pipelines/conda/etc/profile.d/conda.sh\" || echo 'Error: conda/mamba is not installed!' # Install snakemake if missing. # Create environment with snakemake/7.32.3 mamba create -y -c conda-forge -c bioconda -n snakemake snakemake = 7 .32.3 Installation \u00b6 Please ensure the software dependencies listed above are satisfied before getting started with this section. Also, please ensure each of the software dependencies listed above are in your $PATH . You can re-run the same command above to ensure each of the required dependencies are in your $PATH . You can install mpox-seek locally with the following command: # Clone mpox-seek from Github mkdir -p \" ${ HOME :- ~ } /pipelines/mpox-seek\" git clone https://github.com/OpenOmics/mpox-seek.git \" ${ HOME :- ~ } /pipelines/mpox-seek\" # Change your working directory cd mpox-seek/ # Get usage information ./mpox-seek -h # Create an conda enviroment for mpox-seek mamba env create -y --name mpox-seek --file = workflow/envs/mpox.yaml Offline mode \u00b6 The mpox-seek pipeline can be run in an offline mode where external requests are not made at runtime. This will cache and download and remote resources or software containers (if using singlarity). Please note that if you are running the pipeline on Biowulf, you do NOT need to run these next steps. These instructions are for users running the pipeline outside of Biowulf cluster, i.e. on another cluster or locally on a laptop. Download resource bundle \u00b6 Note This pipeline does not have any reference files that need to be downloaded prior to running. As so, everything on this page can be safely ignored! We have bundled all the reference files for the pipeline within our Github repository. All the reference files are located within the resources folder . Please feel free to skip over this section. There are no reference files that need to be downloaded prior to running the pipeline. All reference files are bundled with the pipeline. To download the pipeline's resource bundle, please run the following command: # Dry-run download of the resource bundle, # this will show you what will be downloaded. ./mpox-seek install --ref-path /data/ $USER /refs \\ --force --threads 4 --dry-run # Download the resource bundle, currently # all resources are bundled with the pipeline # so there is nothing to download! ./mpox-seek install --ref-path /data/ $USER /refs \\ --force --threads 4 Please remember the path provided to the --ref-path option above. During the download process, a new child directory called mpox-seek will be created. The path to this directory should be provided to the --resource-bundle option of the run sub command . For more information, please see the documentation for the install sub command . Cache software containers \u00b6 This next step is only applicable for singularity users. If you are using conda/mamba instead of singularity, you can skip over this section. To cache remote software containers, please run the following command: # Dry run to see what will # be pulled from DockerHub ./mpox-seek cache --sif-cache /data/ $USER /cache --dry-run # Cache software containers ./mpox-seek cache --sif-cache /data/ $USER /cache Please remember the path provided to the --sif-cache option above, you will need to provide this path to the --sif-cache option of the run sub command . For more information, please see the documentation for the cache sub command . Cache conda environment \u00b6 This next step is only applicable to conda/mamba users. If you are using singularity instead of conda/mamba, you can skip over this section. By default, when the --use-conda option is provided, a conda environment will be built on the fly. Building a conda environment can be slow, and it also makes exeternal requests so you will need internet access. With that being said, it may make sense to create/cache the conda environment once and re-use it. To cache/create mpox-seek's conda environment, please run the following command: # Create a conda/mamba env # called mpox-seek, you only # need to run this once # on your computer/cluster mamba env create -f workflow/envs/mpox.yaml Running the command above will create a named conda/mamba environment called mpox-seek . Now you can provide --conda-env-name mpox-seek to the run sub command . This will ensure conda/mamba is run in an offline-like mode where no external requests are made at runtime. It will use the local, named conda environment instead of building a new environment on the fly. TLDR \u00b6 Here is everything you need to get quickly get started. This set of instructions assumes you have snakemake and (singularity or conda) already installed on your target system , and that both are in your $PATH . Following the example below, please replace --input .tests/*.gz with your input ONT FastQ files. If you are running the pipeline on Windows, please use the Windows Subsystem for Linux (WSL). Quick Start Other system + singularity offline mode Other system + conda offline mode Biowulf These instructions are for users/admins setting up the pipeline to run ouside of Biowulf in an offline mode. The pipeline can be run in an offline mode for users that do not have internet access with singularity. This mode is useful for researchers running the pipeline in the field on a local laptop running linux. In this example, we will cache/download remote resources in our $HOME directory, but please feel free to point to any other location on you computer or target system. You will need about 4 GB of free diskspace for the download. # Clone mpox-seek from Github git clone https://github.com/OpenOmics/mpox-seek.git # Change your working directory cd mpox-seek/ # Get usage information ./mpox-seek -h # Cache software containers ./mpox-seek cache --sif-cache $HOME /SIFs # Dry run mpox-seek pipeline ./mpox-seek run --input .tests/*.gz --output tmp_01/ \\ --sif-cache $HOME /SIFs --mode local \\ --dry-run # Run mpox-seek pipeline # in offline-mode ./mpox-seek run --input .tests/*.gz --output tmp_01/ \\ --sif-cache $HOME /SIFs --mode local These instructions are for users/admins setting up the pipeline outside of Biowulf. The pipeline can be run in an offline mode for users that do not have internet access with conda/mamba. This mode is useful for researchers running the pipeline in the field on a local laptop running linux, macOS, or Windows Subsystem for Linux (WSL) . In this example, we will download the resource bundle in our $HOME directory, but please feel free to point to any other location on you computer or target system. You will need about 4 GB of free diskspace for the download. # Clone mpox-seek from Github git clone https://github.com/OpenOmics/mpox-seek.git # Change your working directory cd mpox-seek/ # Get usage information ./mpox-seek -h # Add conda/mamba to $PATH which conda \\ || source \" ${ HOME :- ~ } /pipelines/conda/etc/profile.d/conda.sh\" \\ || echo 'Error: conda/mamba is not installed.' # Cache conda environment, # creates a local conda env # called mpox-seek mamba env create -f workflow/envs/mpox.yaml # Activate snakemake conda environment conda activate snakemake # Dry run mpox-seek pipeline to # see what steps/jobs will run ./mpox-seek run --input .tests/*.gz --output tmp_01/ \\ --mode local --use-conda --conda-env-name mpox-seek \\ --additional-strains resources/mpox_additional_strains.fa.gz \\ --batch-id 2024 -04-08 --bootstrap-trees --dry-run # Run mpox-seek pipeline with conda/mamba in # offline-mode, no external requests are made, # all software dependencies are cached locally ./mpox-seek run --input .tests/*.gz --output tmp_01/ \\ --mode local --use-conda --conda-env-name mpox-seek \\ --additional-strains resources/mpox_additional_strains.fa.gz \\ --batch-id 2024 -04-08 --bootstrap-trees If you are running the pipeline on Biowulf, do NOT need to download the resource bundle. These reference files already exist on Biowulf, and the pipeline is setup to automatically use them as needed. Also, we have already cached all of the pipeline's software containers here: /data/OpenOmics/SIFs/ . If you are on Biowulf, you can module load the required dependencies. Whenever the pipeline is provided with the --sif-cache option, it is run in an offline mode. We always recommend providing --sif-cache /data/OpenOmics/SIFs/ when running the pipeline on Biowulf. This avoids issues related to DockerHub request limits if multiple users are concurrently run the pipeline on the cluster. # Grab an interactive node, # do not run on head node! srun -N 1 -n 1 --time = 1 :00:00 --mem = 8gb --cpus-per-task = 2 --pty bash module purge module load singularity snakemake # Clone mpox-seek from Github git clone https://github.com/OpenOmics/mpox-seek.git # Change your working directory cd mpox-seek/ # Get usage information ./mpox-seek -h # Dry run mpox-seek pipeline ./mpox-seek run --input .tests/*.gz --output tmp_01/ \\ --sif-cache /data/OpenOmics/SIFs/ \\ --mode slurm \\ --dry-run # Run mpox-seek pipeline # on Biowulf cluster ./mpox-seek run --input .tests/*.gz --output tmp_01/ \\ --sif-cache /data/OpenOmics/SIFs/ \\ --mode slurm","title":"Setup"},{"location":"setup/#dependencies","text":"Requirements Using Singularity : singularity>=3.5 snakemake<8.0 Using Conda or Mamba : conda/mamba snakemake<8.0 Snakemake must be installed on the target system. Snakemake is a workflow manager that orchestrates each step of the pipeline. The second dependency, i.e singularity OR conda/mamba , handles the dowloading/installation of any remaining software dependencies. By default, the pipeline will utilize singularity; however, the --use-conda option of the run sub command can be provided to use conda/mamba instead of singularity. If possible, we recommend using singularity over conda for reproducibility; however, it is worth noting that singularity and conda produce identical results for this pipeline. If you are running the pipeline on Windows, please use the Windows Subsystem for Linux (WSL) . Singularity can be installed on WSL following these instructions . With that being said, we recommend using conda/mamba over singularity. It is easier to setup and install via using WSL on Windows. Please see the instructions below to install conda/mamba on your system. You can check to see if mpox-seek's software requirements are met by running: # Check if dependencies # are already installed which snakemake || echo 'Error: snakemake is not install.' which singularity \\ || which conda \\ || which mamba \\ || echo 'Error: singularity or conda or mamba are not installed.' # Install conda/mamba if missing. # Create directory to install # everything in $HOME/pipelines. mkdir -p \" ${ HOME :- ~ } /pipelines/conda\" cd \" ${ HOME :- ~ } /pipelines/conda\" # Download the installer for # miniforge and install conda. curl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3- $( uname ) - $( uname -m ) .sh\" # Installs conda, a prompt will # appear, hit ENTER, type yes a # few times, and then enter no # when it asks to initialize # your shell/startup bash \"Miniforge3- $( uname ) - $( uname -m ) .sh\" -f -p $PWD # Start up the conda environment, # if conda/mamba are not installed # follow the instructions above. source \" ${ HOME :- ~ } /pipelines/conda/etc/profile.d/conda.sh\" || echo 'Error: conda/mamba is not installed!' # Install snakemake if missing. # Create environment with snakemake/7.32.3 mamba create -y -c conda-forge -c bioconda -n snakemake snakemake = 7 .32.3","title":"Dependencies"},{"location":"setup/#installation","text":"Please ensure the software dependencies listed above are satisfied before getting started with this section. Also, please ensure each of the software dependencies listed above are in your $PATH . You can re-run the same command above to ensure each of the required dependencies are in your $PATH . You can install mpox-seek locally with the following command: # Clone mpox-seek from Github mkdir -p \" ${ HOME :- ~ } /pipelines/mpox-seek\" git clone https://github.com/OpenOmics/mpox-seek.git \" ${ HOME :- ~ } /pipelines/mpox-seek\" # Change your working directory cd mpox-seek/ # Get usage information ./mpox-seek -h # Create an conda enviroment for mpox-seek mamba env create -y --name mpox-seek --file = workflow/envs/mpox.yaml","title":"Installation"},{"location":"setup/#offline-mode","text":"The mpox-seek pipeline can be run in an offline mode where external requests are not made at runtime. This will cache and download and remote resources or software containers (if using singlarity). Please note that if you are running the pipeline on Biowulf, you do NOT need to run these next steps. These instructions are for users running the pipeline outside of Biowulf cluster, i.e. on another cluster or locally on a laptop.","title":"Offline mode"},{"location":"setup/#download-resource-bundle","text":"Note This pipeline does not have any reference files that need to be downloaded prior to running. As so, everything on this page can be safely ignored! We have bundled all the reference files for the pipeline within our Github repository. All the reference files are located within the resources folder . Please feel free to skip over this section. There are no reference files that need to be downloaded prior to running the pipeline. All reference files are bundled with the pipeline. To download the pipeline's resource bundle, please run the following command: # Dry-run download of the resource bundle, # this will show you what will be downloaded. ./mpox-seek install --ref-path /data/ $USER /refs \\ --force --threads 4 --dry-run # Download the resource bundle, currently # all resources are bundled with the pipeline # so there is nothing to download! ./mpox-seek install --ref-path /data/ $USER /refs \\ --force --threads 4 Please remember the path provided to the --ref-path option above. During the download process, a new child directory called mpox-seek will be created. The path to this directory should be provided to the --resource-bundle option of the run sub command . For more information, please see the documentation for the install sub command .","title":"Download resource bundle"},{"location":"setup/#cache-software-containers","text":"This next step is only applicable for singularity users. If you are using conda/mamba instead of singularity, you can skip over this section. To cache remote software containers, please run the following command: # Dry run to see what will # be pulled from DockerHub ./mpox-seek cache --sif-cache /data/ $USER /cache --dry-run # Cache software containers ./mpox-seek cache --sif-cache /data/ $USER /cache Please remember the path provided to the --sif-cache option above, you will need to provide this path to the --sif-cache option of the run sub command . For more information, please see the documentation for the cache sub command .","title":"Cache software containers"},{"location":"setup/#cache-conda-environment","text":"This next step is only applicable to conda/mamba users. If you are using singularity instead of conda/mamba, you can skip over this section. By default, when the --use-conda option is provided, a conda environment will be built on the fly. Building a conda environment can be slow, and it also makes exeternal requests so you will need internet access. With that being said, it may make sense to create/cache the conda environment once and re-use it. To cache/create mpox-seek's conda environment, please run the following command: # Create a conda/mamba env # called mpox-seek, you only # need to run this once # on your computer/cluster mamba env create -f workflow/envs/mpox.yaml Running the command above will create a named conda/mamba environment called mpox-seek . Now you can provide --conda-env-name mpox-seek to the run sub command . This will ensure conda/mamba is run in an offline-like mode where no external requests are made at runtime. It will use the local, named conda environment instead of building a new environment on the fly.","title":"Cache conda environment"},{"location":"setup/#tldr","text":"Here is everything you need to get quickly get started. This set of instructions assumes you have snakemake and (singularity or conda) already installed on your target system , and that both are in your $PATH . Following the example below, please replace --input .tests/*.gz with your input ONT FastQ files. If you are running the pipeline on Windows, please use the Windows Subsystem for Linux (WSL). Quick Start Other system + singularity offline mode Other system + conda offline mode Biowulf These instructions are for users/admins setting up the pipeline to run ouside of Biowulf in an offline mode. The pipeline can be run in an offline mode for users that do not have internet access with singularity. This mode is useful for researchers running the pipeline in the field on a local laptop running linux. In this example, we will cache/download remote resources in our $HOME directory, but please feel free to point to any other location on you computer or target system. You will need about 4 GB of free diskspace for the download. # Clone mpox-seek from Github git clone https://github.com/OpenOmics/mpox-seek.git # Change your working directory cd mpox-seek/ # Get usage information ./mpox-seek -h # Cache software containers ./mpox-seek cache --sif-cache $HOME /SIFs # Dry run mpox-seek pipeline ./mpox-seek run --input .tests/*.gz --output tmp_01/ \\ --sif-cache $HOME /SIFs --mode local \\ --dry-run # Run mpox-seek pipeline # in offline-mode ./mpox-seek run --input .tests/*.gz --output tmp_01/ \\ --sif-cache $HOME /SIFs --mode local These instructions are for users/admins setting up the pipeline outside of Biowulf. The pipeline can be run in an offline mode for users that do not have internet access with conda/mamba. This mode is useful for researchers running the pipeline in the field on a local laptop running linux, macOS, or Windows Subsystem for Linux (WSL) . In this example, we will download the resource bundle in our $HOME directory, but please feel free to point to any other location on you computer or target system. You will need about 4 GB of free diskspace for the download. # Clone mpox-seek from Github git clone https://github.com/OpenOmics/mpox-seek.git # Change your working directory cd mpox-seek/ # Get usage information ./mpox-seek -h # Add conda/mamba to $PATH which conda \\ || source \" ${ HOME :- ~ } /pipelines/conda/etc/profile.d/conda.sh\" \\ || echo 'Error: conda/mamba is not installed.' # Cache conda environment, # creates a local conda env # called mpox-seek mamba env create -f workflow/envs/mpox.yaml # Activate snakemake conda environment conda activate snakemake # Dry run mpox-seek pipeline to # see what steps/jobs will run ./mpox-seek run --input .tests/*.gz --output tmp_01/ \\ --mode local --use-conda --conda-env-name mpox-seek \\ --additional-strains resources/mpox_additional_strains.fa.gz \\ --batch-id 2024 -04-08 --bootstrap-trees --dry-run # Run mpox-seek pipeline with conda/mamba in # offline-mode, no external requests are made, # all software dependencies are cached locally ./mpox-seek run --input .tests/*.gz --output tmp_01/ \\ --mode local --use-conda --conda-env-name mpox-seek \\ --additional-strains resources/mpox_additional_strains.fa.gz \\ --batch-id 2024 -04-08 --bootstrap-trees If you are running the pipeline on Biowulf, do NOT need to download the resource bundle. These reference files already exist on Biowulf, and the pipeline is setup to automatically use them as needed. Also, we have already cached all of the pipeline's software containers here: /data/OpenOmics/SIFs/ . If you are on Biowulf, you can module load the required dependencies. Whenever the pipeline is provided with the --sif-cache option, it is run in an offline mode. We always recommend providing --sif-cache /data/OpenOmics/SIFs/ when running the pipeline on Biowulf. This avoids issues related to DockerHub request limits if multiple users are concurrently run the pipeline on the cluster. # Grab an interactive node, # do not run on head node! srun -N 1 -n 1 --time = 1 :00:00 --mem = 8gb --cpus-per-task = 2 --pty bash module purge module load singularity snakemake # Clone mpox-seek from Github git clone https://github.com/OpenOmics/mpox-seek.git # Change your working directory cd mpox-seek/ # Get usage information ./mpox-seek -h # Dry run mpox-seek pipeline ./mpox-seek run --input .tests/*.gz --output tmp_01/ \\ --sif-cache /data/OpenOmics/SIFs/ \\ --mode slurm \\ --dry-run # Run mpox-seek pipeline # on Biowulf cluster ./mpox-seek run --input .tests/*.gz --output tmp_01/ \\ --sif-cache /data/OpenOmics/SIFs/ \\ --mode slurm","title":"TLDR"},{"location":"faq/questions/","text":"Frequently Asked Questions \u00b6 This page is still under construction. If you need immediate help, please open an issue on Github!","title":"General Questions"},{"location":"faq/questions/#frequently-asked-questions","text":"This page is still under construction. If you need immediate help, please open an issue on Github!","title":"Frequently Asked Questions"},{"location":"usage/cache/","text":"mpox-seek cache \u00b6 1. About \u00b6 The mpox-seek executable is composed of several inter-related sub commands. Please see mpox-seek -h for all available options. This part of the documentation describes options and concepts for mpox-seek cache sub command in more detail. With minimal configuration, the cache sub command enables you to cache remote software containers from Dockerhub . Caching remote software containers allows the pipeline to run in an offline mode where no requests are made. The cache sub command can also be used to pull our pre-built software container onto a new cluster or target system. These containers are normally pulled onto the filesystem when the pipeline runs; however, due to network issues or DockerHub pull rate limits, it may make sense to pull the resources once so a shared cache can be created. It is worth noting that a singularity cache cannot normally be shared across users. Singularity strictly enforces that a cache is owned by the user. To get around this issue, the cache subcommand can be used to create local SIFs on the filesystem from images on DockerHub. The path of these locally cached SIFs can be passed to the run sub commands --sif-cache option. Caching software containers is fast and easy! In its most basic form, mpox-seek cache only has one required input . 2. Synopsis \u00b6 $ ./mpox-seek cache [--help] [--dry-run] \\ --sif-cache SIF_CACHE The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide a directory to cache remote Docker images via the --sif-cache argument. Once the cache has pipeline completed, the local sif cache can be passed to the --sif-cache option of the mpox-seek run subcomand. This enables the pipeline to run in an offline mode. Use you can always use the -h option for information on a specific command. 2.1 Required Arguments \u00b6 --sif-cache SIF_CACHE Path where a local cache of SIFs will be stored. type: path Any images defined in config/containers.json will be pulled into the local filesystem. The path provided to this option can be passed to the --sif-cache option of the mpox-seek run subcomand. This allows for running the build and run pipelines in an offline mode where no requests are made to external sources. This is useful for avoiding network issues or DockerHub pull rate limits. Please see mpox-seek run for more information. Example: --sif-cache /data/$USER/cache 2.2 Options \u00b6 Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean flag Shows command's synopsis, help message, and an example command Example: --help --dry-run Dry run the pipeline. type: boolean flag Only displays what software container will be cached locally. Does not execute anything! Example: --dry-run 3. Example \u00b6 # Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load singularity snakemake # Step 1.) Dry run to see what will be pulled ./mpox-seek cache --sif-cache /data/ $USER /cache \\ --dry-run # Step 2.) Cache remote resources locally. # This command will NOT automatically submit # a job to the cluster. As so, we recommend # submitting this next command to the cluster # as a job. Download speeds will vary so it # is best to set the wall time a few hours. ./mpox-seek cache --sif-cache /data/ $USER /cache","title":"mpox-seek cache"},{"location":"usage/cache/#mpox-seek-cache","text":"","title":"mpox-seek cache"},{"location":"usage/cache/#1-about","text":"The mpox-seek executable is composed of several inter-related sub commands. Please see mpox-seek -h for all available options. This part of the documentation describes options and concepts for mpox-seek cache sub command in more detail. With minimal configuration, the cache sub command enables you to cache remote software containers from Dockerhub . Caching remote software containers allows the pipeline to run in an offline mode where no requests are made. The cache sub command can also be used to pull our pre-built software container onto a new cluster or target system. These containers are normally pulled onto the filesystem when the pipeline runs; however, due to network issues or DockerHub pull rate limits, it may make sense to pull the resources once so a shared cache can be created. It is worth noting that a singularity cache cannot normally be shared across users. Singularity strictly enforces that a cache is owned by the user. To get around this issue, the cache subcommand can be used to create local SIFs on the filesystem from images on DockerHub. The path of these locally cached SIFs can be passed to the run sub commands --sif-cache option. Caching software containers is fast and easy! In its most basic form, mpox-seek cache only has one required input .","title":"1. About"},{"location":"usage/cache/#2-synopsis","text":"$ ./mpox-seek cache [--help] [--dry-run] \\ --sif-cache SIF_CACHE The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide a directory to cache remote Docker images via the --sif-cache argument. Once the cache has pipeline completed, the local sif cache can be passed to the --sif-cache option of the mpox-seek run subcomand. This enables the pipeline to run in an offline mode. Use you can always use the -h option for information on a specific command.","title":"2. Synopsis"},{"location":"usage/cache/#21-required-arguments","text":"--sif-cache SIF_CACHE Path where a local cache of SIFs will be stored. type: path Any images defined in config/containers.json will be pulled into the local filesystem. The path provided to this option can be passed to the --sif-cache option of the mpox-seek run subcomand. This allows for running the build and run pipelines in an offline mode where no requests are made to external sources. This is useful for avoiding network issues or DockerHub pull rate limits. Please see mpox-seek run for more information. Example: --sif-cache /data/$USER/cache","title":"2.1 Required Arguments"},{"location":"usage/cache/#22-options","text":"Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean flag Shows command's synopsis, help message, and an example command Example: --help --dry-run Dry run the pipeline. type: boolean flag Only displays what software container will be cached locally. Does not execute anything! Example: --dry-run","title":"2.2 Options"},{"location":"usage/cache/#3-example","text":"# Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load singularity snakemake # Step 1.) Dry run to see what will be pulled ./mpox-seek cache --sif-cache /data/ $USER /cache \\ --dry-run # Step 2.) Cache remote resources locally. # This command will NOT automatically submit # a job to the cluster. As so, we recommend # submitting this next command to the cluster # as a job. Download speeds will vary so it # is best to set the wall time a few hours. ./mpox-seek cache --sif-cache /data/ $USER /cache","title":"3. Example"},{"location":"usage/install/","text":"mpox-seek install \u00b6 Note This pipeline does not have any reference files that need to be downloaded prior to running. As so, everything on this page can be safely ignored! We have bundled all the reference files for the pipeline within our Github repository. All the reference files are located within the resources folder . 1. About \u00b6 The mpox-seek executable is composed of several inter-related sub commands. Please see mpox-seek -h for all available options. This part of the documentation describes options and concepts for mpox-seek install sub command in more detail. With minimal configuration, the install sub command enables you to download the pipeline's resource bundle locally. This is necessary when setting up the pipeline on a new target system or cluster. The pipeline uses a set of reference files to process the data. These reference files are required and need to be available on the local file system prior to execution. This command can be used to download any required reference files of the pipeline. Since most resource bundles are very large; we recommend using multiple threads for pulling reference files concurrently. The resource bundle can be very large so please ensure you have sufficent disk space prior to running this sub command. Please Note: The resource bundle requires about 2 GB of available disk space. If you are running the pipeline on the Biowulf cluster, you do NOT need to download the pipeline's resource bundle. It is already accessible to all HPC users. This sub command is for users running the pipeline outside of the Biowulf cluster. Downloading the resource bundle is fast and easy! In its most basic form, mpox-seek install only has one required input . 2. Synopsis \u00b6 $ mpox-seek install [--help] [--dry-run] \\ [--force] [--threads] \\ --ref-path REF_PATH The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide a output directory for the reference file download via the --ref-path argument. Once the download of the resource bundle has completed, a new child directory called mpox-seek will be created. This new directory will contain all of the pipeline's required reference files. The path to this new directory can be passed to the --resource-bundle option of the mpox-seek run subcomand. This allow users outside of Biowulf to run the pipeline. Use you can always use the -h option for information on a specific command. 2.1 Required Arguments \u00b6 --ref-path REF_PATH Path where the resource bundle will be downloaded. type: path Any resouces defined in the 'config/install.json' will be pulled onto the local filesystem. After the files have been downloaded, a new directory with the name mpox-seek will be created. It contains all the required reference files of the pipeline. The path to this new directory can be passed to the run sub command's --resource-bundle option. Please see the run sub command for more information. Example: --ref-path /data/$USER/refs 2.2 Options \u00b6 Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean flag Shows command's synopsis, help message, and an example command Example: --help --dry-run Dry run the pipeline. type: boolean flag Displays what remote resources would be pulled. Does not execute anything! Example: --dry-run --force Force downloads all files. type: boolean flag By default, any files that do not exist locally are pulled; however if a previous instance of an install did not exit gracefully, it may be necessary to forcefully re-download all the files. Example: --force --threads Number of threads to use for concurrent file downloads. type: int default: 2 Max number of threads to use for concurrent file downloads. Example: --threads 12 3. Example \u00b6 # Step 0.) Grab an interactive node, # do not run on head node! srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 24gb --cpus-per-task = 12 --pty bash module purge module load singularity snakemake # Step 1.) Dry-run download of the resource bundle mpox-seek install --ref-path /data/ $USER /refs \\ --force \\ --dry-run \\ --threads 12 # Step 2.) Download the resource bundle, # This command will NOT automatically submit # a job to the cluster. As so, we recommend # submitting this next command to the cluster # as a job. Download speeds will vary so it # is best to set the wall time to 2 days. mpox-seek install --ref-path /data/ $USER /refs \\ --force \\ --threads 12 # Checkout the downloaded files cd /data/ $USER /refs tree mpox-seek # mpox-seek/ # \u251c\u2500\u2500 kronatax_1222 # \u2502 \u2514\u2500\u2500 taxonomy.tab # \u2514\u2500\u2500 NCBI # \u2514\u2500\u2500 viral_genomes_taxid.fa","title":"mpox-seek install"},{"location":"usage/install/#mpox-seek-install","text":"Note This pipeline does not have any reference files that need to be downloaded prior to running. As so, everything on this page can be safely ignored! We have bundled all the reference files for the pipeline within our Github repository. All the reference files are located within the resources folder .","title":"mpox-seek install"},{"location":"usage/install/#1-about","text":"The mpox-seek executable is composed of several inter-related sub commands. Please see mpox-seek -h for all available options. This part of the documentation describes options and concepts for mpox-seek install sub command in more detail. With minimal configuration, the install sub command enables you to download the pipeline's resource bundle locally. This is necessary when setting up the pipeline on a new target system or cluster. The pipeline uses a set of reference files to process the data. These reference files are required and need to be available on the local file system prior to execution. This command can be used to download any required reference files of the pipeline. Since most resource bundles are very large; we recommend using multiple threads for pulling reference files concurrently. The resource bundle can be very large so please ensure you have sufficent disk space prior to running this sub command. Please Note: The resource bundle requires about 2 GB of available disk space. If you are running the pipeline on the Biowulf cluster, you do NOT need to download the pipeline's resource bundle. It is already accessible to all HPC users. This sub command is for users running the pipeline outside of the Biowulf cluster. Downloading the resource bundle is fast and easy! In its most basic form, mpox-seek install only has one required input .","title":"1. About"},{"location":"usage/install/#2-synopsis","text":"$ mpox-seek install [--help] [--dry-run] \\ [--force] [--threads] \\ --ref-path REF_PATH The synopsis for each command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide a output directory for the reference file download via the --ref-path argument. Once the download of the resource bundle has completed, a new child directory called mpox-seek will be created. This new directory will contain all of the pipeline's required reference files. The path to this new directory can be passed to the --resource-bundle option of the mpox-seek run subcomand. This allow users outside of Biowulf to run the pipeline. Use you can always use the -h option for information on a specific command.","title":"2. Synopsis"},{"location":"usage/install/#21-required-arguments","text":"--ref-path REF_PATH Path where the resource bundle will be downloaded. type: path Any resouces defined in the 'config/install.json' will be pulled onto the local filesystem. After the files have been downloaded, a new directory with the name mpox-seek will be created. It contains all the required reference files of the pipeline. The path to this new directory can be passed to the run sub command's --resource-bundle option. Please see the run sub command for more information. Example: --ref-path /data/$USER/refs","title":"2.1 Required Arguments"},{"location":"usage/install/#22-options","text":"Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean flag Shows command's synopsis, help message, and an example command Example: --help --dry-run Dry run the pipeline. type: boolean flag Displays what remote resources would be pulled. Does not execute anything! Example: --dry-run --force Force downloads all files. type: boolean flag By default, any files that do not exist locally are pulled; however if a previous instance of an install did not exit gracefully, it may be necessary to forcefully re-download all the files. Example: --force --threads Number of threads to use for concurrent file downloads. type: int default: 2 Max number of threads to use for concurrent file downloads. Example: --threads 12","title":"2.2 Options"},{"location":"usage/install/#3-example","text":"# Step 0.) Grab an interactive node, # do not run on head node! srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 24gb --cpus-per-task = 12 --pty bash module purge module load singularity snakemake # Step 1.) Dry-run download of the resource bundle mpox-seek install --ref-path /data/ $USER /refs \\ --force \\ --dry-run \\ --threads 12 # Step 2.) Download the resource bundle, # This command will NOT automatically submit # a job to the cluster. As so, we recommend # submitting this next command to the cluster # as a job. Download speeds will vary so it # is best to set the wall time to 2 days. mpox-seek install --ref-path /data/ $USER /refs \\ --force \\ --threads 12 # Checkout the downloaded files cd /data/ $USER /refs tree mpox-seek # mpox-seek/ # \u251c\u2500\u2500 kronatax_1222 # \u2502 \u2514\u2500\u2500 taxonomy.tab # \u2514\u2500\u2500 NCBI # \u2514\u2500\u2500 viral_genomes_taxid.fa","title":"3. Example"},{"location":"usage/run/","text":"mpox-seek run \u00b6 1. About \u00b6 The mpox-seek executable is composed of several inter-related sub commands. Please see mpox-seek -h for all available options. This part of the documentation describes options and concepts for mpox-seek run sub command in more detail. With minimal configuration, the run sub command enables you to start running mpox-seek pipeline. Setting up the mpox-seek pipeline is fast and easy! In its most basic form, mpox-seek run only has two required inputs . 2. Synopsis \u00b6 $ mpox-seek run [--help] \\ [--dry-run] [--job-name JOB_NAME] [--mode {slurm,local}] \\ [--sif-cache SIF_CACHE] [--singularity-cache SINGULARITY_CACHE] \\ [--silent] [--threads THREADS] [--tmp-dir TMP_DIR] \\ [--resource-bundle RESOURCE_BUNDLE] [--use-conda] \\ [--conda-env-name CONDA_ENV_NAME] \\ [--additional-strains ADDITIONAL_STRAINS] \\ [--batch-id BATCH_ID] \\ [--bootstrap-trees] \\ [--msa-tool {mafft,viralmsa}] \\ [--plot-coverage] \\ [--tree-tool {raxml-ng,fasttree}] \\ [--whole-genome-sequencing] \\ --input INPUT [INPUT ...] \\ --output OUTPUT The synopsis for each command shows its arguments and their usage. Optional arguments are shown in square brackets. A user must provide a list of FastQ (globbing is supported) to analyze via --input argument and an output directory to store results via --output argument. Use you can always use the -h option for information on a specific command. 2.1 Required arguments \u00b6 Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code. --input INPUT [INPUT ...] Input Oxford Nanopore FastQ files(s). type: file(s) One or more FastQ files can be provided. From the command-line, each input file should seperated by a space. Globbing is supported! This makes selecting FastQ files easy. Input FastQ files should always be gzipp-ed. If a sample has multiple fastq files for different barcodes, the pipeline expects each barcoded FastQ file endwith the following extension: _N.fastq.gz , where N is a number. Internally, the pipeline will concatenate each of these FastQ files prior to processing the data. Here is an example of an input sample with multiple barcode sequences: S1_0.fastq.gz , S1_1.fastq.gz , S1_2.fastq.gz , S1_3.fastq.gz . Given this barcoded sample, the pipeline will create the following concatenated FastQ file: S1.fastq.gz . Example: --input .tests/*.fastq.gz --output OUTPUT Path to an output directory. type: path This location is where the pipeline will create all of its output files, also known as the pipeline's working directory. If the provided output directory does not exist, it will be created automatically. Example: --output /data/$USER/mpox-seek_out 2.2 Analysis options \u00b6 Each of the following arguments are optional, and do not need to be provided. --additional-strains ADDITIONAL_STRAINS Genomic fasta file of additional monekypox strains to add to the phylogenetic tree. type: FASTA file default: none This is a genomic fasta file of additional monekypox strains to add to the phylogenetic tree. By default, a phylogenetic tree is build with your input samples and the reference genome , see \"mpox_pcr_sequence\" in \" config/genome.json \" for the path to this file. When this option is provided a phylogenetic tree containing your input samples, the reference genome, and any additional monkeypox strain the provided file are built. We have provided a genomic fasta file of additional strains with mpox-seek. Please see \" resources/mpox_additional_strains.fa.gz \" for more information. This file can be provided directly to this option. We highly recommended using this option with the --batch-id option below to avoid any files from being overwritten between runs of the pipeline. Example: resources/mpox_additional_strains.fa.gz --batch-id BATCH_ID Unique identifer to associate with a batch of samples. type: string default: none This option can be provided to ensure that project-level output files are not over-written between runs of the pipeline. As so, it is good to always provide this option. By default, project-level files in the \"project\" will get over-written between pipeline runs if this option is not provided. Any identifer provided to this option will be used to create a sub-directory in the project folder. This ensures project-level files (which are unique) will not get over-written as new data/samples are processed. A unique batch id should be provided between runs. This batch id should be composed of alphanumeric characters and it should not contain a white space or tab characters. Here is a list of valid or acceptable characters: aA-Zz , 0-9 , - , _ . Example: --batch-id \"2024-04-01\" --bootstrap-trees Computes branch support by bootstraping data. type: boolean flag default: false This option will empirically compute the support for each branch by bootstrapping the data. If this flag is provided, raxml-ng is run in an all-in-one (ML search + bootstrapping) mode via its --all option. Branch supports, calculated by bootstrapping, will be added to the best scoring tree. By default, the pipeline will not created a tree with transferred bootstrapped supports. Example: --bootstrap-trees --msa-tool {mafft,viralmsa} Select a tool for multiple sequence alignment. type: string default: mafft Set this option to perform multiple sequence alignment (MSA) using one of the provided tools. This option allows a user to select an alternative tool or method for performing MSA. By default, the pipeline will use mafft ; however, for whole-genome sequencing data viralmsa is recommended. Currently, there are two different tools/options available: mafft or viralmsa . Here is an overview of each tool. mafft Performs global multiple sequence alignment. This is the slower of the two options; however, it produces the best results. This is the default msa tool if this option is not provided. If you have PCR amplicon data, this is the recommended option for performing MSA. viralmsa Performs reference guided multiple sequence alignment. This is the faster of the two options; however, it produces rooted output relative to the provided reference genome. If you are running the pipeline with complete viral sequences and the whole genome sequencing option (i.e --whole-genome-sequencing ), we recommened using viralmsa over mafft. Viralmsa will be orders of magnitiude faster than mafft, and it can scale to hundreds or thousands of samples/additional strains. Example: --msa-tool mafft --plot-coverage Plots coverage of each sample. type: boolean flag default: false This option will plot coverage along the reference genome. If this flag is provided, per-sample plots of raw coverage will be created. This plot can be useful for identifying samples or regions of the reference genome with low coverage. By default, the pipeline will not create any coverage plots. Example: --plot-coverage --tree-tool {raxml-ng,fasttree} Select a tool for building a phylogentic tree. type: string default: raxml-ng Set this option to build a phylogentic tree using one of the provided tools. This option allows a user to build a phylogentic tree using the selected tool. This option allows a user to select an alternative tool or method for building a phylogentic tree. Currently, there are two different tools/options available: raxmlng , fasttree . Here is a short description of each available tool. raxml-ng Builds a tree using maximum-likelihood (ML) optimality criterion. This is the slower of the two options; however, it produces the best results. This is the default tree building tool if this option is not provided. If you have PCR amplicon data, this is the recommended option for building a phylogentic tree. fasttree Builds a tree using fasttree's variant of a neighbor-joining. This is the faster of the two options; however, it may not produce the most optimal topology. If you are running the pipeline with complete viral sequences and the whole genome sequencing option (i.e --whole-genome -sequencing ), we recommened using fasttree over raxml-ng . Fasttree will be orders of magnitiude faster than raxml-ng, and it can scale to hundreds or thousands of samples/ additional strains. Benchmarking has also shown that bootstrapping with complete viral genomes and an additional strains file can be completed with fasttree whereas with raxml-ng, it would take days or longer to perform. Example: --tree-tool raxml-ng --whole-genome-sequencing Runs the pipeline in WGS mode. type: boolean flag default: false This flag is used to indicate that the input FastQ files are whole genome sequences. By default, the pipeline will assume that the input FastQ files are amplicon sequences. This option is only required if the input FastQ files contain whole genome sequences. If provided, the pipeline will align reads against the entire monkeypox genome. It is also worh noting that if this option is provided, we highly recommend also providing the following options : --msa-tool viralmsa and --tree-tool fasttree due to the size of the input data. The viralmsa and fasttree are better optimized for large datasets, such as whole genome sequences. Example: --whole-genome-sequencing 2.3 Orchestration options \u00b6 Each of the following arguments are optional, and do not need to be provided. --dry-run Dry run the pipeline. type: boolean flag Displays what steps in the pipeline remain or will be run. Does not execute anything! Example: --dry-run --silent Silence standard output. type: boolean flag Reduces the amount of information directed to standard output when submitting master job to the job scheduler. Only the job id of the master job is returned. Example: --silent --mode {local,slurm} Execution Method. type: string default: local Execution Method. Defines the mode or method of execution. Vaild mode options include: slurm or local. local Local executions will run serially on compute instance, laptop, or desktop computer. This is useful for testing, debugging, or when a users does not have access to a high performance computing environment. If this option is not provided, it will default to a this mode of execution. This is the correct mode of execution if you are running the pipeline on a laptop or a local desktop computer. slurm The slurm execution method will submit jobs to the SLURM workload manager . This method will submit jobs to a SLURM HPC cluster using sbatch. It is recommended running the pipeline in this mode as it will be significantly faster; however, this mode of execution can only be provided if the pipeline is being run from a SLURM HPC cluster. By default, the pipeline runs in a local mode of execution. If you are running this pipeline on a laptop or desktop compute, please use the local mode of execution. Example: --mode local --job-name JOB_NAME Set the name of the pipeline's master job. type: string default: pl:mpox-seek When submitting the pipeline to a job scheduler, like SLURM, this option always you to set the name of the pipeline's master job. By default, the name of the pipeline's master job is set to \"pl:mpox-seek\". Example: --job-name pl_id-42 --singularity-cache SINGULARITY_CACHE Overrides the $SINGULARITY_CACHEDIR environment variable. type: path default: --output OUTPUT/.singularity Singularity will cache image layers pulled from remote registries. This ultimately speeds up the process of pull an image from DockerHub if an image layer already exists in the singularity cache directory. By default, the cache is set to the value provided to the --output argument. Please note that this cache cannot be shared across users. Singularity strictly enforces you own the cache directory and will return a non-zero exit code if you do not own the cache directory! See the --sif-cache option to create a shareable resource. Example: --singularity-cache /data/$USER/.singularity --sif-cache SIF_CACHE Path where a local cache of SIFs are stored. type: path Uses a local cache of SIFs on the filesystem. This SIF cache can be shared across users if permissions are set correctly. If a SIF does not exist in the SIF cache, the image will be pulled from Dockerhub and a warning message will be displayed. The mpox-seek cache subcommand can be used to create a local SIF cache. Please see mpox-seek cache for more information. This command is extremely useful for avoiding DockerHub pull rate limits. It also remove any potential errors that could occur due to network issues or DockerHub being temporarily unavailable. We recommend running mpox-seek with this option when ever possible. Example: -sif-cache /data/$USER/SIFs --threads THREADS Max number of threads for each process. type: int default: 2 Max number of threads for each process. This option is more applicable when running the pipeline with --mode local . It is recommended setting this vaule to the maximum number of CPUs available on the host machine. Example: --threads 12 --tmp-dir TMP_DIR Max number of threads for each process. type: path default: /lscratch/$SLURM_JOBID Path on the file system for writing temporary output files. By default, the temporary directory is set to '/lscratch/$SLURM_JOBID' for backwards compatibility with the NIH's Biowulf cluster; however, if you are running the pipeline on another cluster, this option will need to be specified. Ideally, this path should point to a dedicated location on the filesystem for writing tmp files. On many systems, this location is set to somewhere in /scratch. If you need to inject a variable into this string that should NOT be expanded, please quote this options value in single quotes. Example: --tmp-dir /scratch/$USER/ --resource-bundle RESOURCE_BUNDLE Path to a resource bundle downloaded with the install sub command. type: path At the current moment, the pipeline does not need any external resources/reference files to be downloaded prior to running. All the pipeline's reference files have been bundled within the github repository. They can be found within the resources folder . As so, this option should not be provided at run time. Example: --resource-bundle /data/$USER/refs/mpox-seek --use-conda Use Conda/mamba instead of Singularity. type: boolean flag Use Conda/Mamba instead of Singularity. By default, the pipeline uses singularity for handling required software dependencies. This option overrides that behavior, and it will use Conda/mamba instead of Singularity. The use of Singuarity and Conda are mutually exclusive. Please note that conda and mamba must be in your $PATH prior to running the pipeline. This option will build a conda environment on the fly prior to the pipeline's execution. As so, this step requires internet access. To run mpox-seek in an offline mode with conda, please see the --conda-env-name option below. Example: --use-conda --conda-env-name CONDA_ENV_NAME Use an existing conda environment. type: str Use an existing conda environment. This option allows mpox-seek to run with conda in an offline mode. If you are using conda without this option, the pipeline will build a conda environment on the fly prior to the its execution. Building a conda environment can sometimes be slow, as it downloads dependencies from the internet, so it may make sense to build it once and re-use it. This will also allow you to use conda/mamba in an offline mode. If you have already built a named conda environment with the supplied yaml file, then you can directly use it with this option. Please provide the name of the conda environment that was specifically built for the mpox-seek pipeline. To create a reusable conda/mamba environment with the name mpox-seek , please run the following mamba command: # Creates a reusable conda # environment called mpox-seek mamba env create -f workflow/envs/mpox.yaml Example: --conda-env-name mpox-seek 2.4 Miscellaneous options \u00b6 Each of the following arguments are optional, and do not need to be provided. -h, --help Display Help. type: boolean flag Shows command's synopsis, help message, and an example command Example: --help 3. Example \u00b6 The example below shows how run the pipeline locally using conda/mamba. If you have already created a mpox-seek conda environment, please use feel free to also add the following option: --conda-env-name mpox-seek . To create a re-usable, named conda environment for the pipeline, please run the following command: mamba env create -f workflow/envs/mpox.yaml . For detailed setup instructions, please see our setup page . 3.1 Targeted, PCR amplicon sequencing data \u00b6 # Step 1.) Activate your conda environment, # assumes its installed in home directory. # May need to change this depending on # where you installed conda/mamba. . ${ HOME } /conda/etc/profile.d/conda.sh conda activate snakemake # Step 2A.) Dry-run the pipeline, this # will show what steps will run. ./mpox-seek run --input .tests/*.fastq.gz \\ --output pcr_mpox-seek_output \\ --additional-strains resources/mpox_additional_strains.fa.gz \\ --batch-id \" $( date '+%Y-%m-%d-%H-%M' ) \" \\ --bootstrap-trees \\ --mode local \\ --use-conda \\ --dry-run # Step 2B.) Run the mpox-seek pipeline, # Create a tree with additional # strains of interest and adds a # unique batch identifer to project- # level files to ensure no over # writting of files occurs, format: # YYYY-MM-DD-HH-MM. Support for each # branch is calculated via bootstrapping. # The pipeline will default to using # the mafft and raxml-ng tools for # multiple sequence alignment and # phylogenetic tree construction. # Mafft and raxml-ng are recommended # for amplicon data. ./mpox-seek run --input .tests/*.fastq.gz \\ --output pcr_mpox-seek_output \\ --additional-strains resources/mpox_additional_strains.fa.gz \\ --batch-id \" $( date '+%Y-%m-%d-%H-%M' ) \" \\ --bootstrap-trees \\ --use-conda \\ --mode local 3.1 Complete, whole-genome sequencing data \u00b6 Mpox-seek is designed to work with both amplicon and whole-genome sequencing data. If you have complete viral sequences, please run the pipeline with the following options below. Please note: An additional strains file for complete viral sequences has not been bundled with the pipeline; however, you can create your own file with the same format as the one provided with the pipeline. # Step 1.) Activate your conda environment, # assumes its installed in home directory. # May need to change this depending on # where you installed conda/mamba. . ${ HOME } /conda/etc/profile.d/conda.sh conda activate snakemake # Step 2A.) Dry-run the pipeline, this # will show what steps will run. ./mpox-seek run --input .tests/*.fastq.gz \\ --output wgs_mpox-seek_output \\ --additional-strains mpox_wgs_additional_strains.fa.gz \\ --batch-id \" $( date '+%Y-%m-%d-%H-%M' ) \" \\ --bootstrap-trees \\ --msa-tool viralmsa \\ --tree-tool fasttree \\ --whole-genome-sequencing \\ --mode local \\ --use-conda \\ --dry-run # Step 2B.) Run the mpox-seek pipeline, # Create a tree with additional # strains of interest and adds a # unique batch identifer to project- # level files to ensure no over # writting of files occurs, format: # YYYY-MM-DD-HH-MM. Support for each # branch is calculated via bootstrapping. # For WGS data, we recommend using # the viralmsa and fasttree tools for # multiple sequence alignment and # phylogenetic tree construction. ./mpox-seek run --input .tests/*.fastq.gz \\ --output wgs_mpox-seek_output \\ --additional-strains mpox_wgs_additional_strains.fa.gz \\ --batch-id \" $( date '+%Y-%m-%d-%H-%M' ) \" \\ --bootstrap-trees \\ --msa-tool viralmsa \\ --tree-tool fasttree \\ --whole-genome-sequencing \\ --use-conda \\ --mode local","title":"mpox-seek run"},{"location":"usage/run/#mpox-seek-run","text":"","title":"mpox-seek run"},{"location":"usage/run/#1-about","text":"The mpox-seek executable is composed of several inter-related sub commands. Please see mpox-seek -h for all available options. This part of the documentation describes options and concepts for mpox-seek run sub command in more detail. With minimal configuration, the run sub command enables you to start running mpox-seek pipeline. Setting up the mpox-seek pipeline is fast and easy! In its most basic form, mpox-seek run only has two required inputs .","title":"1. About"},{"location":"usage/run/#2-synopsis","text":"$ mpox-seek run [--help] \\ [--dry-run] [--job-name JOB_NAME] [--mode {slurm,local}] \\ [--sif-cache SIF_CACHE] [--singularity-cache SINGULARITY_CACHE] \\ [--silent] [--threads THREADS] [--tmp-dir TMP_DIR] \\ [--resource-bundle RESOURCE_BUNDLE] [--use-conda] \\ [--conda-env-name CONDA_ENV_NAME] \\ [--additional-strains ADDITIONAL_STRAINS] \\ [--batch-id BATCH_ID] \\ [--bootstrap-trees] \\ [--msa-tool {mafft,viralmsa}] \\ [--plot-coverage] \\ [--tree-tool {raxml-ng,fasttree}] \\ [--whole-genome-sequencing] \\ --input INPUT [INPUT ...] \\ --output OUTPUT The synopsis for each command shows its arguments and their usage. Optional arguments are shown in square brackets. A user must provide a list of FastQ (globbing is supported) to analyze via --input argument and an output directory to store results via --output argument. Use you can always use the -h option for information on a specific command.","title":"2. Synopsis"},{"location":"usage/run/#21-required-arguments","text":"Each of the following arguments are required. Failure to provide a required argument will result in a non-zero exit-code. --input INPUT [INPUT ...] Input Oxford Nanopore FastQ files(s). type: file(s) One or more FastQ files can be provided. From the command-line, each input file should seperated by a space. Globbing is supported! This makes selecting FastQ files easy. Input FastQ files should always be gzipp-ed. If a sample has multiple fastq files for different barcodes, the pipeline expects each barcoded FastQ file endwith the following extension: _N.fastq.gz , where N is a number. Internally, the pipeline will concatenate each of these FastQ files prior to processing the data. Here is an example of an input sample with multiple barcode sequences: S1_0.fastq.gz , S1_1.fastq.gz , S1_2.fastq.gz , S1_3.fastq.gz . Given this barcoded sample, the pipeline will create the following concatenated FastQ file: S1.fastq.gz . Example: --input .tests/*.fastq.gz --output OUTPUT Path to an output directory. type: path This location is where the pipeline will create all of its output files, also known as the pipeline's working directory. If the provided output directory does not exist, it will be created automatically. Example: --output /data/$USER/mpox-seek_out","title":"2.1 Required arguments"},{"location":"usage/run/#22-analysis-options","text":"Each of the following arguments are optional, and do not need to be provided. --additional-strains ADDITIONAL_STRAINS Genomic fasta file of additional monekypox strains to add to the phylogenetic tree. type: FASTA file default: none This is a genomic fasta file of additional monekypox strains to add to the phylogenetic tree. By default, a phylogenetic tree is build with your input samples and the reference genome , see \"mpox_pcr_sequence\" in \" config/genome.json \" for the path to this file. When this option is provided a phylogenetic tree containing your input samples, the reference genome, and any additional monkeypox strain the provided file are built. We have provided a genomic fasta file of additional strains with mpox-seek. Please see \" resources/mpox_additional_strains.fa.gz \" for more information. This file can be provided directly to this option. We highly recommended using this option with the --batch-id option below to avoid any files from being overwritten between runs of the pipeline. Example: resources/mpox_additional_strains.fa.gz --batch-id BATCH_ID Unique identifer to associate with a batch of samples. type: string default: none This option can be provided to ensure that project-level output files are not over-written between runs of the pipeline. As so, it is good to always provide this option. By default, project-level files in the \"project\" will get over-written between pipeline runs if this option is not provided. Any identifer provided to this option will be used to create a sub-directory in the project folder. This ensures project-level files (which are unique) will not get over-written as new data/samples are processed. A unique batch id should be provided between runs. This batch id should be composed of alphanumeric characters and it should not contain a white space or tab characters. Here is a list of valid or acceptable characters: aA-Zz , 0-9 , - , _ . Example: --batch-id \"2024-04-01\" --bootstrap-trees Computes branch support by bootstraping data. type: boolean flag default: false This option will empirically compute the support for each branch by bootstrapping the data. If this flag is provided, raxml-ng is run in an all-in-one (ML search + bootstrapping) mode via its --all option. Branch supports, calculated by bootstrapping, will be added to the best scoring tree. By default, the pipeline will not created a tree with transferred bootstrapped supports. Example: --bootstrap-trees --msa-tool {mafft,viralmsa} Select a tool for multiple sequence alignment. type: string default: mafft Set this option to perform multiple sequence alignment (MSA) using one of the provided tools. This option allows a user to select an alternative tool or method for performing MSA. By default, the pipeline will use mafft ; however, for whole-genome sequencing data viralmsa is recommended. Currently, there are two different tools/options available: mafft or viralmsa . Here is an overview of each tool. mafft Performs global multiple sequence alignment. This is the slower of the two options; however, it produces the best results. This is the default msa tool if this option is not provided. If you have PCR amplicon data, this is the recommended option for performing MSA. viralmsa Performs reference guided multiple sequence alignment. This is the faster of the two options; however, it produces rooted output relative to the provided reference genome. If you are running the pipeline with complete viral sequences and the whole genome sequencing option (i.e --whole-genome-sequencing ), we recommened using viralmsa over mafft. Viralmsa will be orders of magnitiude faster than mafft, and it can scale to hundreds or thousands of samples/additional strains. Example: --msa-tool mafft --plot-coverage Plots coverage of each sample. type: boolean flag default: false This option will plot coverage along the reference genome. If this flag is provided, per-sample plots of raw coverage will be created. This plot can be useful for identifying samples or regions of the reference genome with low coverage. By default, the pipeline will not create any coverage plots. Example: --plot-coverage --tree-tool {raxml-ng,fasttree} Select a tool for building a phylogentic tree. type: string default: raxml-ng Set this option to build a phylogentic tree using one of the provided tools. This option allows a user to build a phylogentic tree using the selected tool. This option allows a user to select an alternative tool or method for building a phylogentic tree. Currently, there are two different tools/options available: raxmlng , fasttree . Here is a short description of each available tool. raxml-ng Builds a tree using maximum-likelihood (ML) optimality criterion. This is the slower of the two options; however, it produces the best results. This is the default tree building tool if this option is not provided. If you have PCR amplicon data, this is the recommended option for building a phylogentic tree. fasttree Builds a tree using fasttree's variant of a neighbor-joining. This is the faster of the two options; however, it may not produce the most optimal topology. If you are running the pipeline with complete viral sequences and the whole genome sequencing option (i.e --whole-genome -sequencing ), we recommened using fasttree over raxml-ng . Fasttree will be orders of magnitiude faster than raxml-ng, and it can scale to hundreds or thousands of samples/ additional strains. Benchmarking has also shown that bootstrapping with complete viral genomes and an additional strains file can be completed with fasttree whereas with raxml-ng, it would take days or longer to perform. Example: --tree-tool raxml-ng --whole-genome-sequencing Runs the pipeline in WGS mode. type: boolean flag default: false This flag is used to indicate that the input FastQ files are whole genome sequences. By default, the pipeline will assume that the input FastQ files are amplicon sequences. This option is only required if the input FastQ files contain whole genome sequences. If provided, the pipeline will align reads against the entire monkeypox genome. It is also worh noting that if this option is provided, we highly recommend also providing the following options : --msa-tool viralmsa and --tree-tool fasttree due to the size of the input data. The viralmsa and fasttree are better optimized for large datasets, such as whole genome sequences. Example: --whole-genome-sequencing","title":"2.2 Analysis options"},{"location":"usage/run/#23-orchestration-options","text":"Each of the following arguments are optional, and do not need to be provided. --dry-run Dry run the pipeline. type: boolean flag Displays what steps in the pipeline remain or will be run. Does not execute anything! Example: --dry-run --silent Silence standard output. type: boolean flag Reduces the amount of information directed to standard output when submitting master job to the job scheduler. Only the job id of the master job is returned. Example: --silent --mode {local,slurm} Execution Method. type: string default: local Execution Method. Defines the mode or method of execution. Vaild mode options include: slurm or local. local Local executions will run serially on compute instance, laptop, or desktop computer. This is useful for testing, debugging, or when a users does not have access to a high performance computing environment. If this option is not provided, it will default to a this mode of execution. This is the correct mode of execution if you are running the pipeline on a laptop or a local desktop computer. slurm The slurm execution method will submit jobs to the SLURM workload manager . This method will submit jobs to a SLURM HPC cluster using sbatch. It is recommended running the pipeline in this mode as it will be significantly faster; however, this mode of execution can only be provided if the pipeline is being run from a SLURM HPC cluster. By default, the pipeline runs in a local mode of execution. If you are running this pipeline on a laptop or desktop compute, please use the local mode of execution. Example: --mode local --job-name JOB_NAME Set the name of the pipeline's master job. type: string default: pl:mpox-seek When submitting the pipeline to a job scheduler, like SLURM, this option always you to set the name of the pipeline's master job. By default, the name of the pipeline's master job is set to \"pl:mpox-seek\". Example: --job-name pl_id-42 --singularity-cache SINGULARITY_CACHE Overrides the $SINGULARITY_CACHEDIR environment variable. type: path default: --output OUTPUT/.singularity Singularity will cache image layers pulled from remote registries. This ultimately speeds up the process of pull an image from DockerHub if an image layer already exists in the singularity cache directory. By default, the cache is set to the value provided to the --output argument. Please note that this cache cannot be shared across users. Singularity strictly enforces you own the cache directory and will return a non-zero exit code if you do not own the cache directory! See the --sif-cache option to create a shareable resource. Example: --singularity-cache /data/$USER/.singularity --sif-cache SIF_CACHE Path where a local cache of SIFs are stored. type: path Uses a local cache of SIFs on the filesystem. This SIF cache can be shared across users if permissions are set correctly. If a SIF does not exist in the SIF cache, the image will be pulled from Dockerhub and a warning message will be displayed. The mpox-seek cache subcommand can be used to create a local SIF cache. Please see mpox-seek cache for more information. This command is extremely useful for avoiding DockerHub pull rate limits. It also remove any potential errors that could occur due to network issues or DockerHub being temporarily unavailable. We recommend running mpox-seek with this option when ever possible. Example: -sif-cache /data/$USER/SIFs --threads THREADS Max number of threads for each process. type: int default: 2 Max number of threads for each process. This option is more applicable when running the pipeline with --mode local . It is recommended setting this vaule to the maximum number of CPUs available on the host machine. Example: --threads 12 --tmp-dir TMP_DIR Max number of threads for each process. type: path default: /lscratch/$SLURM_JOBID Path on the file system for writing temporary output files. By default, the temporary directory is set to '/lscratch/$SLURM_JOBID' for backwards compatibility with the NIH's Biowulf cluster; however, if you are running the pipeline on another cluster, this option will need to be specified. Ideally, this path should point to a dedicated location on the filesystem for writing tmp files. On many systems, this location is set to somewhere in /scratch. If you need to inject a variable into this string that should NOT be expanded, please quote this options value in single quotes. Example: --tmp-dir /scratch/$USER/ --resource-bundle RESOURCE_BUNDLE Path to a resource bundle downloaded with the install sub command. type: path At the current moment, the pipeline does not need any external resources/reference files to be downloaded prior to running. All the pipeline's reference files have been bundled within the github repository. They can be found within the resources folder . As so, this option should not be provided at run time. Example: --resource-bundle /data/$USER/refs/mpox-seek --use-conda Use Conda/mamba instead of Singularity. type: boolean flag Use Conda/Mamba instead of Singularity. By default, the pipeline uses singularity for handling required software dependencies. This option overrides that behavior, and it will use Conda/mamba instead of Singularity. The use of Singuarity and Conda are mutually exclusive. Please note that conda and mamba must be in your $PATH prior to running the pipeline. This option will build a conda environment on the fly prior to the pipeline's execution. As so, this step requires internet access. To run mpox-seek in an offline mode with conda, please see the --conda-env-name option below. Example: --use-conda --conda-env-name CONDA_ENV_NAME Use an existing conda environment. type: str Use an existing conda environment. This option allows mpox-seek to run with conda in an offline mode. If you are using conda without this option, the pipeline will build a conda environment on the fly prior to the its execution. Building a conda environment can sometimes be slow, as it downloads dependencies from the internet, so it may make sense to build it once and re-use it. This will also allow you to use conda/mamba in an offline mode. If you have already built a named conda environment with the supplied yaml file, then you can directly use it with this option. Please provide the name of the conda environment that was specifically built for the mpox-seek pipeline. To create a reusable conda/mamba environment with the name mpox-seek , please run the following mamba command: # Creates a reusable conda # environment called mpox-seek mamba env create -f workflow/envs/mpox.yaml Example: --conda-env-name mpox-seek","title":"2.3 Orchestration options"},{"location":"usage/run/#24-miscellaneous-options","text":"Each of the following arguments are optional, and do not need to be provided. -h, --help Display Help. type: boolean flag Shows command's synopsis, help message, and an example command Example: --help","title":"2.4 Miscellaneous options"},{"location":"usage/run/#3-example","text":"The example below shows how run the pipeline locally using conda/mamba. If you have already created a mpox-seek conda environment, please use feel free to also add the following option: --conda-env-name mpox-seek . To create a re-usable, named conda environment for the pipeline, please run the following command: mamba env create -f workflow/envs/mpox.yaml . For detailed setup instructions, please see our setup page .","title":"3. Example"},{"location":"usage/run/#31-targeted-pcr-amplicon-sequencing-data","text":"# Step 1.) Activate your conda environment, # assumes its installed in home directory. # May need to change this depending on # where you installed conda/mamba. . ${ HOME } /conda/etc/profile.d/conda.sh conda activate snakemake # Step 2A.) Dry-run the pipeline, this # will show what steps will run. ./mpox-seek run --input .tests/*.fastq.gz \\ --output pcr_mpox-seek_output \\ --additional-strains resources/mpox_additional_strains.fa.gz \\ --batch-id \" $( date '+%Y-%m-%d-%H-%M' ) \" \\ --bootstrap-trees \\ --mode local \\ --use-conda \\ --dry-run # Step 2B.) Run the mpox-seek pipeline, # Create a tree with additional # strains of interest and adds a # unique batch identifer to project- # level files to ensure no over # writting of files occurs, format: # YYYY-MM-DD-HH-MM. Support for each # branch is calculated via bootstrapping. # The pipeline will default to using # the mafft and raxml-ng tools for # multiple sequence alignment and # phylogenetic tree construction. # Mafft and raxml-ng are recommended # for amplicon data. ./mpox-seek run --input .tests/*.fastq.gz \\ --output pcr_mpox-seek_output \\ --additional-strains resources/mpox_additional_strains.fa.gz \\ --batch-id \" $( date '+%Y-%m-%d-%H-%M' ) \" \\ --bootstrap-trees \\ --use-conda \\ --mode local","title":"3.1 Targeted, PCR amplicon sequencing data"},{"location":"usage/run/#31-complete-whole-genome-sequencing-data","text":"Mpox-seek is designed to work with both amplicon and whole-genome sequencing data. If you have complete viral sequences, please run the pipeline with the following options below. Please note: An additional strains file for complete viral sequences has not been bundled with the pipeline; however, you can create your own file with the same format as the one provided with the pipeline. # Step 1.) Activate your conda environment, # assumes its installed in home directory. # May need to change this depending on # where you installed conda/mamba. . ${ HOME } /conda/etc/profile.d/conda.sh conda activate snakemake # Step 2A.) Dry-run the pipeline, this # will show what steps will run. ./mpox-seek run --input .tests/*.fastq.gz \\ --output wgs_mpox-seek_output \\ --additional-strains mpox_wgs_additional_strains.fa.gz \\ --batch-id \" $( date '+%Y-%m-%d-%H-%M' ) \" \\ --bootstrap-trees \\ --msa-tool viralmsa \\ --tree-tool fasttree \\ --whole-genome-sequencing \\ --mode local \\ --use-conda \\ --dry-run # Step 2B.) Run the mpox-seek pipeline, # Create a tree with additional # strains of interest and adds a # unique batch identifer to project- # level files to ensure no over # writting of files occurs, format: # YYYY-MM-DD-HH-MM. Support for each # branch is calculated via bootstrapping. # For WGS data, we recommend using # the viralmsa and fasttree tools for # multiple sequence alignment and # phylogenetic tree construction. ./mpox-seek run --input .tests/*.fastq.gz \\ --output wgs_mpox-seek_output \\ --additional-strains mpox_wgs_additional_strains.fa.gz \\ --batch-id \" $( date '+%Y-%m-%d-%H-%M' ) \" \\ --bootstrap-trees \\ --msa-tool viralmsa \\ --tree-tool fasttree \\ --whole-genome-sequencing \\ --use-conda \\ --mode local","title":"3.1 Complete, whole-genome sequencing data"},{"location":"usage/unlock/","text":"mpox-seek unlock \u00b6 1. About \u00b6 The mpox-seek executable is composed of several inter-related sub commands. Please see mpox-seek -h for all available options. This part of the documentation describes options and concepts for mpox-seek unlock sub command in more detail. With minimal configuration, the unlock sub command enables you to unlock a pipeline output directory. If the pipeline fails ungracefully, it maybe required to unlock the working directory before proceeding again. Snakemake will inform a user when it maybe necessary to unlock a working directory with an error message stating: Error: Directory cannot be locked . Please verify that the pipeline is not running before running this command. If the pipeline is currently running, the workflow manager will report the working directory is locked. The is the default behavior of snakemake, and it is normal. Do NOT run this command if the pipeline is still running! Please kill the master job and it's child jobs prior to running this command. Unlocking mpox-seek pipeline output directory is fast and easy! In its most basic form, mpox-seek unlock only has one required input . 2. Synopsis \u00b6 $ ./mpox-seek unlock [-h] --output OUTPUT The synopsis for this command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide an output directory to unlock via --output argument. After running the unlock sub command, you can resume the build or run pipeline from where it left off by re-running it. Use you can always use the -h option for information on a specific command. 2.1 Required Arguments \u00b6 --output OUTPUT Output directory to unlock. type: path Path to a previous run's output directory. This will remove a lock on the working directory. Please verify that the pipeline is not running before running this command. Example: --output /data/$USER/mpox-seek_out 2.2 Options \u00b6 Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help 3. Example \u00b6 # Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load singularity snakemake # Step 1.) Unlock a pipeline output directory mpox-seek unlock --output /data/ $USER /output","title":"mpox-seek unlock"},{"location":"usage/unlock/#mpox-seek-unlock","text":"","title":"mpox-seek unlock"},{"location":"usage/unlock/#1-about","text":"The mpox-seek executable is composed of several inter-related sub commands. Please see mpox-seek -h for all available options. This part of the documentation describes options and concepts for mpox-seek unlock sub command in more detail. With minimal configuration, the unlock sub command enables you to unlock a pipeline output directory. If the pipeline fails ungracefully, it maybe required to unlock the working directory before proceeding again. Snakemake will inform a user when it maybe necessary to unlock a working directory with an error message stating: Error: Directory cannot be locked . Please verify that the pipeline is not running before running this command. If the pipeline is currently running, the workflow manager will report the working directory is locked. The is the default behavior of snakemake, and it is normal. Do NOT run this command if the pipeline is still running! Please kill the master job and it's child jobs prior to running this command. Unlocking mpox-seek pipeline output directory is fast and easy! In its most basic form, mpox-seek unlock only has one required input .","title":"1. About"},{"location":"usage/unlock/#2-synopsis","text":"$ ./mpox-seek unlock [-h] --output OUTPUT The synopsis for this command shows its parameters and their usage. Optional parameters are shown in square brackets. A user must provide an output directory to unlock via --output argument. After running the unlock sub command, you can resume the build or run pipeline from where it left off by re-running it. Use you can always use the -h option for information on a specific command.","title":"2. Synopsis"},{"location":"usage/unlock/#21-required-arguments","text":"--output OUTPUT Output directory to unlock. type: path Path to a previous run's output directory. This will remove a lock on the working directory. Please verify that the pipeline is not running before running this command. Example: --output /data/$USER/mpox-seek_out","title":"2.1 Required Arguments"},{"location":"usage/unlock/#22-options","text":"Each of the following arguments are optional and do not need to be provided. -h, --help Display Help. type: boolean Shows command's synopsis, help message, and an example command Example: --help","title":"2.2 Options"},{"location":"usage/unlock/#3-example","text":"# Step 0.) Grab an interactive node (do not run on head node) srun -N 1 -n 1 --time = 12 :00:00 -p interactive --mem = 8gb --cpus-per-task = 4 --pty bash module purge module load singularity snakemake # Step 1.) Unlock a pipeline output directory mpox-seek unlock --output /data/ $USER /output","title":"3. Example"}]}